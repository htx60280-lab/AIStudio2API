import asyncio
import json
import os
import random
import secrets
import time
from typing import Optional, Tuple, Callable, AsyncGenerator
from asyncio import Event, Future
from fastapi import HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from playwright.async_api import Page as AsyncPage, Locator, Error as PlaywrightAsyncError, expect as expect_async
from config import *
from config.timeouts import STREAM_CHUNK_SIZE
from models import ChatCompletionRequest, ClientDisconnectedError
from browser import switch_ai_studio_model, save_error_snapshot
from .utils import validate_chat_request, prepare_combined_prompt, generate_sse_chunk, generate_sse_stop_chunk, use_stream_response, calculate_usage_stats, request_manager, calculate_stream_max_retries
from .abort_detector import AbortSignalDetector, AbortSignalHandler
from browser.page_controller import PageController

async def _initialize_request_context(req_id: str, request: ChatCompletionRequest) -> dict:
    from server import logger, page_instance, is_page_ready, parsed_model_list, current_ai_studio_model_id, model_switching_lock, page_params_cache, params_cache_lock
    request_manager.register_request(req_id, {'model': request.model, 'stream': request.stream, 'message_count': len(request.messages)})
    logger.info(f'[{req_id}] ğŸš€ å¼€å§‹è¯·æ±‚ | Model: {request.model} | Stream: {request.stream}')
    context = {'logger': logger, 'page': page_instance, 'is_page_ready': is_page_ready, 'parsed_model_list': parsed_model_list, 'current_ai_studio_model_id': current_ai_studio_model_id, 'model_switching_lock': model_switching_lock, 'page_params_cache': page_params_cache, 'params_cache_lock': params_cache_lock, 'is_streaming': request.stream, 'model_actually_switched': False, 'requested_model': request.model, 'model_id_to_use': None, 'needs_model_switching': False}
    return context

async def _analyze_model_requirements(req_id: str, context: dict, request: ChatCompletionRequest) -> dict:
    logger = context['logger']
    current_ai_studio_model_id = context['current_ai_studio_model_id']
    parsed_model_list = context['parsed_model_list']
    requested_model = request.model
    if requested_model and requested_model != MODEL_NAME:
        requested_model_id = requested_model.split('/')[-1]
        logger.info(f'[{req_id}] è¯·æ±‚ä½¿ç”¨æ¨¡å‹: {requested_model_id}')
        if parsed_model_list:
            valid_model_ids = [m.get('id') for m in parsed_model_list]
            if requested_model_id not in valid_model_ids:
                raise HTTPException(status_code=400, detail=f"[{req_id}] Invalid model '{requested_model_id}'. Available models: {', '.join(valid_model_ids)}")
        context['model_id_to_use'] = requested_model_id
        if current_ai_studio_model_id != requested_model_id:
            context['needs_model_switching'] = True
            logger.info(f'[{req_id}] éœ€è¦åˆ‡æ¢æ¨¡å‹: å½“å‰={current_ai_studio_model_id} -> ç›®æ ‡={requested_model_id}')
    return context

async def _test_client_connection(req_id: str, http_request: Request) -> bool:
    from server import logger
    state = getattr(http_request, 'state', None)
    if state:
        try:
            now = time.time()
            last_ts = getattr(state, '_last_disconnect_check_ts', 0.0)
            if now - last_ts < 0.2:
                return getattr(state, '_last_disconnect_check_ok', True)
        except Exception:
            pass

    def _cache_result(result: bool) -> bool:
        if state:
            try:
                state._last_disconnect_check_ts = time.time()
                state._last_disconnect_check_ok = result
            except Exception:
                pass
        return result
    try:
        is_disconnected = await http_request.is_disconnected()
        if is_disconnected:
            logger.info(f'[{req_id}] ğŸš¨ æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ - is_disconnected() = True')
            return _cache_result(False)
        if hasattr(http_request, '_receive'):
            import asyncio
            try:
                receive_task = asyncio.create_task(http_request._receive())
                done, pending = await asyncio.wait([receive_task], timeout=0.05)
                if done:
                    message = receive_task.result()
                    message_type = message.get('type', 'unknown')
                    logger.info(f"[{req_id}] ğŸ” æ”¶åˆ°ASGIæ¶ˆæ¯: type={message_type}, body_size={len(message.get('body', b''))}, more_body={message.get('more_body', 'N/A')}")
                    if message_type == 'http.disconnect':
                        logger.info(f'[{req_id}] ğŸš¨ Cherry Studioåœæ­¢ä¿¡å· - http.disconnect')
                        return _cache_result(False)
                    if message_type in ['websocket.disconnect', 'websocket.close']:
                        logger.info(f'[{req_id}] ğŸš¨ WebSocketæ–­å¼€ä¿¡å· - {message_type}')
                        return _cache_result(False)
                    if message_type == 'http.request':
                        body = message.get('body', b'')
                        more_body = message.get('more_body', True)
                        if body == b'' and (not more_body):
                            logger.info(f'[{req_id}] ğŸš¨ ç©ºbodyåœæ­¢ä¿¡å·')
                            return _cache_result(False)
                        if body:
                            body_str = body.decode('utf-8', errors='ignore').lower()
                            if any((stop_word in body_str for stop_word in ['abort', 'cancel', 'stop'])):
                                logger.info(f'[{req_id}] ğŸš¨ æ£€æµ‹åˆ°åœæ­¢å…³é”®è¯åœ¨bodyä¸­: {body_str[:100]}')
                                return _cache_result(False)
                else:
                    for task in pending:
                        task.cancel()
                        try:
                            await task
                        except asyncio.CancelledError:
                            pass
            except asyncio.TimeoutError:
                pass
            except Exception as e:
                logger.warning(f'[{req_id}] ASGIæ¶ˆæ¯æ£€æµ‹å¼‚å¸¸: {e}')
                error_msg = str(e).lower()
                if any((keyword in error_msg for keyword in ['disconnect', 'closed', 'abort', 'cancel', 'reset', 'broken'])):
                    logger.info(f'[{req_id}] ğŸš¨ å¼‚å¸¸è¡¨ç¤ºæ–­å¼€è¿æ¥: {e}')
                    return _cache_result(False)
        try:
            if hasattr(http_request, 'scope'):
                scope = http_request.scope
                transport = scope.get('transport')
                if transport:
                    if hasattr(transport, 'is_closing') and transport.is_closing():
                        logger.info(f'[{req_id}] ğŸš¨ ä¼ è¾“å±‚æ­£åœ¨å…³é—­')
                        return _cache_result(False)
                    if hasattr(transport, 'is_closed') and transport.is_closed():
                        logger.info(f'[{req_id}] ğŸš¨ ä¼ è¾“å±‚å·²å…³é—­')
                        return _cache_result(False)
        except Exception:
            pass
        return _cache_result(True)
    except Exception as e:
        logger.warning(f'[{req_id}] è¿æ¥æ£€æµ‹æ€»å¼‚å¸¸: {e}')
        return _cache_result(False)

async def _setup_disconnect_monitoring(req_id: str, http_request: Request, result_future: Future, page: AsyncPage) -> Tuple[Event, asyncio.Task, Callable]:
    from server import logger
    client_disconnected_event = Event()
    page_controller = PageController(page, logger, req_id)
    logger.info(f'[{req_id}] ğŸš€ åˆ›å»ºå®¢æˆ·ç«¯æ–­å¼€ç›‘æ§ä»»åŠ¡')

    async def listen_for_disconnect():
        logger.info(f'[{req_id}] ğŸ‘‚ å¯åŠ¨é•¿è¿æ¥æ–­å¼€ç›‘å¬ (Event-Driven)...')
        try:
            while not client_disconnected_event.is_set():
                # ç›´æ¥ç­‰å¾… ASGI æ¶ˆæ¯ï¼Œä¸å†è½®è¯¢
                message = await http_request.receive()
                if message['type'] == 'http.disconnect':
                    logger.warning(f'[{req_id}] ğŸ”Œ æ”¶åˆ° http.disconnect ä¿¡å·')
                    client_disconnected_event.set()
                    if not result_future.done():
                        result_future.set_exception(HTTPException(status_code=499, detail=f'[{req_id}] å®¢æˆ·ç«¯å…³é—­äº†è¯·æ±‚'))
                    
                    logger.info(f'[{req_id}] ğŸ›‘ å®¢æˆ·ç«¯æ–­å¼€ï¼Œè§¦å‘é¡µé¢åœæ­¢ç”Ÿæˆ...')
                    try:
                        # å®šä¹‰ä¸€ä¸ªç®€æ˜“çš„æ£€æŸ¥å‡½æ•°ï¼Œé¿å…å¾ªç¯ä¾èµ–
                        def simple_disconnect_check(stage=''): return False
                        await page_controller.stop_generation(simple_disconnect_check)
                        logger.info(f'[{req_id}] âœ… é¡µé¢åœæ­¢ç”Ÿæˆå‘½ä»¤æ‰§è¡ŒæˆåŠŸ')
                    except Exception as stop_err:
                        logger.error(f'[{req_id}] âŒ é¡µé¢åœæ­¢ç”Ÿæˆå¤±è´¥: {stop_err}')
                    break
                # å¦‚æœæ”¶åˆ°å…¶ä»–ç±»å‹çš„æ¶ˆæ¯ (æå°‘è§ï¼Œå› ä¸ºBodyå·²è¢«è¯»å–)ï¼Œç»§ç»­ç­‰å¾…
        except asyncio.CancelledError:
            logger.info(f'[{req_id}] ğŸ“› æ–­å¼€ç›‘å¬ä»»åŠ¡è¢«å–æ¶ˆ')
        except Exception as e:
            # æŸäº›æƒ…å†µä¸‹ receive() å¯èƒ½ä¼šå› ä¸ºè¿æ¥æ—©å·²æ–­å¼€è€ŒæŠ›å‡ºå¼‚å¸¸
            logger.warning(f'[{req_id}] âŒ æ–­å¼€ç›‘å¬æ•è·å¼‚å¸¸ (å¯èƒ½è¿æ¥å·²å…³é—­): {e}')
            client_disconnected_event.set()
            if not result_future.done():
                result_future.set_exception(HTTPException(status_code=499, detail=f'[{req_id}] Client connection lost: {e}'))
    
    disconnect_check_task = asyncio.create_task(listen_for_disconnect())
    logger.info(f'[{req_id}] âœ… ç›‘æ§ä»»åŠ¡å·²åˆ›å»ºå¹¶å¯åŠ¨: {disconnect_check_task}')

    def check_client_disconnected(stage: str=''):
        if request_manager.is_cancelled(req_id):
            logger.info(f"[{req_id}] åœ¨ '{stage}' æ£€æµ‹åˆ°è¯·æ±‚è¢«ç”¨æˆ·å–æ¶ˆã€‚")
            raise ClientDisconnectedError(f'[{req_id}] Request cancelled by user at stage: {stage}')
        if client_disconnected_event.is_set():
            logger.info(f"[{req_id}] åœ¨ '{stage}' æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ã€‚")
            raise ClientDisconnectedError(f'[{req_id}] Client disconnected at stage: {stage}')
        return False
    return (client_disconnected_event, disconnect_check_task, check_client_disconnected)

async def _validate_page_status(req_id: str, context: dict, check_client_disconnected: Callable) -> None:
    page = context['page']
    is_page_ready = context['is_page_ready']
    if not page or page.is_closed() or (not is_page_ready):
        raise HTTPException(status_code=503, detail=f'[{req_id}] AI Studio é¡µé¢ä¸¢å¤±æˆ–æœªå°±ç»ªã€‚', headers={'Retry-After': '30'})
    check_client_disconnected('Initial Page Check')

async def _handle_model_switching(req_id: str, context: dict, check_client_disconnected: Callable) -> dict:
    if not context['needs_model_switching']:
        return context
    logger = context['logger']
    page = context['page']
    model_switching_lock = context['model_switching_lock']
    model_id_to_use = context['model_id_to_use']
    import server
    async with model_switching_lock:
        if server.current_ai_studio_model_id != model_id_to_use:
            logger.info(f'[{req_id}] ğŸ”„ åˆ‡æ¢æ¨¡å‹: {server.current_ai_studio_model_id} -> {model_id_to_use}')
            switch_success = await switch_ai_studio_model(page, model_id_to_use, req_id)
            if switch_success:
                server.current_ai_studio_model_id = model_id_to_use
                context['model_actually_switched'] = True
                context['current_ai_studio_model_id'] = model_id_to_use
                logger.info(f'[{req_id}] âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸ')
            else:
                await _handle_model_switch_failure(req_id, page, model_id_to_use, server.current_ai_studio_model_id, logger)
    return context

async def _handle_model_switch_failure(req_id: str, page: AsyncPage, model_id_to_use: str, model_before_switch: str, logger) -> None:
    import server
    logger.warning(f'[{req_id}] âŒ æ¨¡å‹åˆ‡æ¢è‡³ {model_id_to_use} å¤±è´¥ã€‚')
    server.current_ai_studio_model_id = model_before_switch
    raise HTTPException(status_code=422, detail=f"[{req_id}] æœªèƒ½åˆ‡æ¢åˆ°æ¨¡å‹ '{model_id_to_use}'ã€‚è¯·ç¡®ä¿æ¨¡å‹å¯ç”¨ã€‚")

async def _handle_parameter_cache(req_id: str, context: dict) -> None:
    logger = context['logger']
    params_cache_lock = context['params_cache_lock']
    page_params_cache = context['page_params_cache']
    current_ai_studio_model_id = context['current_ai_studio_model_id']
    model_actually_switched = context['model_actually_switched']
    async with params_cache_lock:
        cached_model_for_params = page_params_cache.get('last_known_model_id_for_params')
        if model_actually_switched or current_ai_studio_model_id != cached_model_for_params:
            page_params_cache.clear()
            page_params_cache['last_known_model_id_for_params'] = current_ai_studio_model_id

async def _prepare_and_validate_request(req_id: str, request: ChatCompletionRequest, check_client_disconnected: Callable) -> Tuple[str, str, list]:
    from server import logger
    try:
        validate_chat_request(request.messages, req_id)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f'[{req_id}] æ— æ•ˆè¯·æ±‚: {e}')
    system_prompt, prepared_prompt, final_image_list = prepare_combined_prompt(request.messages, req_id)
    check_client_disconnected('After Prompt Prep')
    if final_image_list:
        logger.info(f'[{req_id}] ğŸ–¼ï¸ å‡†å¤‡ä¸Šä¼  {len(final_image_list)} å¼ å›¾ç‰‡')
    return (system_prompt, prepared_prompt, final_image_list)

async def _handle_response_processing(req_id: str, request: ChatCompletionRequest, page: AsyncPage, context: dict, result_future: Future, submit_button_locator: Locator, check_client_disconnected: Callable, disconnect_check_task: Optional[asyncio.Task]) -> Optional[Tuple[Event, Locator, Callable]]:
    from server import logger
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')
    stream_port = os.environ.get('STREAM_PORT')
    use_stream = stream_port != '0'
    if use_stream:
        return await _handle_auxiliary_stream_response(req_id, request, context, result_future, submit_button_locator, check_client_disconnected, disconnect_check_task)
    else:
        return await _handle_playwright_response(req_id, request, page, context, result_future, submit_button_locator, check_client_disconnected)

async def _handle_auxiliary_stream_response(req_id: str, request: ChatCompletionRequest, context: dict, result_future: Future, submit_button_locator: Locator, check_client_disconnected: Callable, disconnect_check_task: Optional[asyncio.Task]) -> Optional[Tuple[Event, Locator, Callable]]:
    from server import logger
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')


    if is_streaming:
        try:
            completion_event = Event()
            max_stream_retries = calculate_stream_max_retries(request.messages)
            logger.info(f"[{req_id}] åŠ¨æ€æµå¼è¶…æ—¶è®¾ç½® - Max Retries: {max_stream_retries}")

            async def create_stream_generator_from_helper(event_to_set: Event, task_to_cancel: Optional[asyncio.Task], page_controller: PageController) -> AsyncGenerator[str, None]:
                skip_button_stop_event = asyncio.Event()
                skip_monitor_task = asyncio.create_task(page_controller.continuously_handle_skip_button(skip_button_stop_event, check_client_disconnected))
                last_reason_pos = 0
                last_body_pos = 0
                model_name_for_stream = current_ai_studio_model_id or MODEL_NAME
                chat_completion_id = f'{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}-{random.randint(100, 999)}'
                created_timestamp = int(time.time())
                full_reasoning_content = ''
                full_body_content = ''
                data_receiving = False
                try:
                    async for raw_data in use_stream_response(req_id, max_stream_retries):
                        data_receiving = True
                        try:
                            check_client_disconnected(f'æµå¼ç”Ÿæˆå™¨å¾ªç¯ ({req_id}): ')
                        except ClientDisconnectedError:
                            logger.info(f'[{req_id}] ğŸš¨ æµå¼ç”Ÿæˆå™¨æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼ˆé€šè¿‡äº‹ä»¶ï¼‰')
                            if data_receiving and (not event_to_set.is_set()):
                                logger.info(f'[{req_id}] æ•°æ®æ¥æ”¶ä¸­å®¢æˆ·ç«¯æ–­å¼€ï¼Œç«‹å³è®¾ç½®doneä¿¡å·')
                                event_to_set.set()
                            try:
                                stop_chunk = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [{'index': 0, 'delta': {'role': 'assistant'}, 'finish_reason': 'stop', 'native_finish_reason': 'stop'}]}
                                yield f"data: {json.dumps(stop_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                                yield 'data: [DONE]\n\n'
                            except Exception:
                                pass
                            break
                        try:
                            import server
                            if hasattr(server, 'current_http_requests') and req_id in server.current_http_requests:
                                current_http_request = server.current_http_requests[req_id]
                                is_connected = await _test_client_connection(req_id, current_http_request)
                                if not is_connected:
                                    logger.info(f'[{req_id}] ğŸš¨ æµå¼ç”Ÿæˆå™¨ç‹¬ç«‹æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼')
                                    if data_receiving and (not event_to_set.is_set()):
                                        event_to_set.set()
                                    try:
                                        stop_chunk = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [{'index': 0, 'delta': {'role': 'assistant'}, 'finish_reason': 'stop', 'native_finish_reason': 'stop'}]}
                                        yield f"data: {json.dumps(stop_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                                        yield 'data: [DONE]\n\n'
                                    except Exception:
                                        pass
                                    break
                        except Exception as direct_check_err:
                            pass
                        if isinstance(raw_data, str):
                            try:
                                data = json.loads(raw_data)
                            except json.JSONDecodeError:
                                logger.warning(f'[{req_id}] æ— æ³•è§£ææµæ•°æ®JSON: {raw_data}')
                                continue
                        elif isinstance(raw_data, dict):
                            data = raw_data
                            if data.get('error') == 'rate_limit':
                                logger.warning(f"[{req_id}] ğŸš¨ æ¥æ”¶åˆ°æ¥è‡ªä»£ç†çš„é€Ÿç‡é™åˆ¶ä¿¡å·: {data}")
                                try:
                                    error_chunk = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [{'index': 0, 'delta': {'role': 'assistant', 'content': f"\n\n[System: Rate Limit Exceeded - {data.get('detail', 'Quota exceeded')}]"}, 'finish_reason': 'stop', 'native_finish_reason': 'stop'}]}
                                    yield f"data: {json.dumps(error_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                                except: pass
                                if not event_to_set.is_set():
                                    event_to_set.set()
                                break
                        else:
                            logger.warning(f'[{req_id}] æœªçŸ¥çš„æµæ•°æ®ç±»å‹: {type(raw_data)}')
                            continue
                        if not isinstance(data, dict):
                            logger.warning(f'[{req_id}] æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹: {data}')
                            continue
                        reason = data.get('reason', '')
                        body = data.get('body', '')
                        done = data.get('done', False)
                        function = data.get('function', [])
                        if reason:
                            full_reasoning_content = reason
                        if body:
                            full_body_content = body
                        if len(reason) > last_reason_pos:
                            output = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [{'index': 0, 'delta': {'role': 'assistant', 'content': None, 'reasoning_content': reason[last_reason_pos:]}, 'finish_reason': None, 'native_finish_reason': None}]}
                            last_reason_pos = len(reason)
                            yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                        if len(body) > last_body_pos:
                            finish_reason_val = None
                            if done:
                                finish_reason_val = 'stop'
                            delta_content = {'role': 'assistant', 'content': body[last_body_pos:]}
                            choice_item = {'index': 0, 'delta': delta_content, 'finish_reason': finish_reason_val, 'native_finish_reason': finish_reason_val}
                            if done and function and (len(function) > 0):
                                tool_calls_list = []
                                for func_idx, function_call_data in enumerate(function):
                                    tool_calls_list.append({'id': f'call_{secrets.token_hex(12)}', 'index': func_idx, 'type': 'function', 'function': {'name': function_call_data['name'], 'arguments': json.dumps(function_call_data['params'])}})
                                delta_content['tool_calls'] = tool_calls_list
                                choice_item['finish_reason'] = 'tool_calls'
                                choice_item['native_finish_reason'] = 'tool_calls'
                                delta_content['content'] = None
                            output = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [choice_item]}
                            last_body_pos = len(body)
                            yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                        elif done:
                            if function and len(function) > 0:
                                delta_content = {'role': 'assistant', 'content': None}
                                tool_calls_list = []
                                for func_idx, function_call_data in enumerate(function):
                                    tool_calls_list.append({'id': f'call_{secrets.token_hex(12)}', 'index': func_idx, 'type': 'function', 'function': {'name': function_call_data['name'], 'arguments': json.dumps(function_call_data['params'])}})
                                delta_content['tool_calls'] = tool_calls_list
                                choice_item = {'index': 0, 'delta': delta_content, 'finish_reason': 'tool_calls', 'native_finish_reason': 'tool_calls'}
                            else:
                                choice_item = {'index': 0, 'delta': {'role': 'assistant'}, 'finish_reason': 'stop', 'native_finish_reason': 'stop'}
                            output = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [choice_item]}
                            yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                    
                    # Late Rate Limit Check
                    late_check_wait = 2.0 if len(full_body_content) < 50 else 0.2
                    if late_check_wait > 0.5:
                         logger.info(f"[{req_id}] å†…å®¹è¾ƒçŸ­ ({len(full_body_content)}), ç­‰å¾… {late_check_wait}s æ£€æŸ¥å»¶è¿Ÿ Rate Limit")
                    await asyncio.sleep(late_check_wait)
                    try:
                        from server import STREAM_QUEUE
                        import queue
                        if STREAM_QUEUE:
                            while True:
                                try:
                                    msg = STREAM_QUEUE.get_nowait()
                                    if isinstance(msg, dict) and msg.get('error') == 'rate_limit':
                                        logger.warning(f"[{req_id}] ğŸš¨ æ•è·åˆ°å»¶è¿Ÿçš„ Rate Limit ä¿¡å·: {msg}")
                                        try:
                                            error_chunk = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [{'index': 0, 'delta': {'role': 'assistant', 'content': f"\n\n[System: Rate Limit Exceeded - {msg.get('detail', 'Quota exceeded')}]"}, 'finish_reason': 'stop', 'native_finish_reason': 'stop'}]}
                                            yield f"data: {json.dumps(error_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                                        except: pass
                                except queue.Empty:
                                    break
                    except Exception as e:
                        logger.error(f"[{req_id}] Late check failed: {e}")
                    
                except ClientDisconnectedError as disconnect_err:
                    abort_handler = AbortSignalHandler()
                    disconnect_info = abort_handler.handle_error(disconnect_err, req_id)
                    logger.info(f'[{req_id}] æµå¼ç”Ÿæˆå™¨ä¸­æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥')
                    logger.info(f'[{req_id}] åœæ­¢åŸå› åˆ†æ: {disconnect_info}')
                    if data_receiving and (not event_to_set.is_set()):
                        logger.info(f'[{req_id}] å®¢æˆ·ç«¯æ–­å¼€å¼‚å¸¸å¤„ç†ä¸­ç«‹å³è®¾ç½®doneä¿¡å·')
                        event_to_set.set()
                except Exception as e:
                    abort_handler = AbortSignalHandler()
                    error_info = abort_handler.handle_error(e, req_id)
                    if error_info['stop_reason'] in ['user_abort', 'client_disconnect']:
                        logger.info(f'[{req_id}] æ£€æµ‹åˆ°åœæ­¢ä¿¡å·: {error_info}')
                        if data_receiving and (not event_to_set.is_set()):
                            event_to_set.set()
                    else:
                        logger.error(f'[{req_id}] æµå¼ç”Ÿæˆå™¨å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}', exc_info=True)
                    try:
                        error_chunk = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [{'index': 0, 'delta': {'role': 'assistant', 'content': f'\n\n[é”™è¯¯: {str(e)}]'}, 'finish_reason': 'stop', 'native_finish_reason': 'stop'}]}
                        yield f"data: {json.dumps(error_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                    except Exception:
                        pass
                finally:
                    logger.info(f"[{req_id}] æµå¼ç”Ÿæˆå™¨ç»“æŸï¼Œæ­£åœ¨åœæ­¢ 'Skip' æŒ‰é’®ç›‘æ§...")
                    skip_button_stop_event.set()
                    try:
                        await asyncio.wait_for(skip_monitor_task, timeout=2.0)
                        logger.info(f"[{req_id}] 'Skip' æŒ‰é’®ç›‘æ§ä»»åŠ¡å·²æˆåŠŸæ¸…ç†ã€‚")
                    except asyncio.TimeoutError:
                        logger.warning(f"[{req_id}] 'Skip' æŒ‰é’®ç›‘æ§ä»»åŠ¡å…³é—­è¶…æ—¶ã€‚")
                    except Exception as e_clean_skip:
                        logger.error(f"[{req_id}] æ¸…ç† 'Skip' æŒ‰é’®ç›‘æ§ä»»åŠ¡æ—¶å‡ºé”™: {e_clean_skip}")
                    try:
                        usage_stats = calculate_usage_stats([msg.model_dump() for msg in request.messages], full_body_content, full_reasoning_content)
                        logger.info(f'[{req_id}] è®¡ç®—çš„tokenä½¿ç”¨ç»Ÿè®¡: {usage_stats}')
                        final_chunk = {'id': chat_completion_id, 'object': 'chat.completion.chunk', 'model': model_name_for_stream, 'created': created_timestamp, 'choices': [{'index': 0, 'delta': {}, 'finish_reason': 'stop', 'native_finish_reason': 'stop'}], 'usage': usage_stats}
                        yield f"data: {json.dumps(final_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                        logger.info(f'[{req_id}] å·²å‘é€å¸¦usageç»Ÿè®¡çš„æœ€ç»ˆchunk')
                    except Exception as usage_err:
                        logger.error(f'[{req_id}] è®¡ç®—æˆ–å‘é€usageç»Ÿè®¡æ—¶å‡ºé”™: {usage_err}')
                    try:
                        logger.info(f'[{req_id}] æµå¼ç”Ÿæˆå™¨å®Œæˆï¼Œå‘é€ [DONE] æ ‡è®°')
                        yield 'data: [DONE]\n\n'
                    except Exception as done_err:
                        logger.error(f'[{req_id}] å‘é€ [DONE] æ ‡è®°æ—¶å‡ºé”™: {done_err}')
                    if not event_to_set.is_set():
                        event_to_set.set()
                        logger.info(f'[{req_id}] æµå¼ç”Ÿæˆå™¨å®Œæˆäº‹ä»¶å·²è®¾ç½®')
                    logger.info(f'[{req_id}] æµå¼ç”Ÿæˆå™¨ç»“æŸï¼Œå¼€å§‹æ¸…ç†èµ„æº...')
                    import server
                    if hasattr(server, 'current_http_requests'):
                        server.current_http_requests.pop(req_id, None)
                        logger.info(f'[{req_id}] âœ… å·²æ¸…ç†å…¨å±€HTTPè¯·æ±‚çŠ¶æ€')
                    if task_to_cancel and (not task_to_cancel.done()):
                        task_to_cancel.cancel()
                        logger.info(f'[{req_id}] âœ… å·²å‘é€å–æ¶ˆä¿¡å·åˆ°ç›‘æ§ä»»åŠ¡')
                    else:
                        logger.info(f'[{req_id}] âœ… ç›‘æ§ä»»åŠ¡æ— éœ€å–æ¶ˆï¼ˆå¯èƒ½å·²å®Œæˆæˆ–ä¸å­˜åœ¨ï¼‰')
            page_controller = PageController(context['page'], logger, req_id)
            stream_gen_func = create_stream_generator_from_helper(completion_event, disconnect_check_task, page_controller)
            if not result_future.done():
                result_future.set_result(StreamingResponse(stream_gen_func, media_type='text/event-stream'))
            elif not completion_event.is_set():
                completion_event.set()
            return (completion_event, submit_button_locator, check_client_disconnected)
        except Exception as e:
            logger.error(f'[{req_id}] ä»é˜Ÿåˆ—è·å–æµå¼æ•°æ®æ—¶å‡ºé”™: {e}', exc_info=True)
            if completion_event and (not completion_event.is_set()):
                completion_event.set()
            raise
    else:
        content = None
        reasoning_content = None
        functions = None
        final_data_from_aux_stream = None
        max_stream_retries = calculate_stream_max_retries(request.messages)
        logger.info(f"[{req_id}] åŠ¨æ€éæµå¼è¶…æ—¶è®¾ç½® - Max Retries: {max_stream_retries}")

        async for raw_data in use_stream_response(req_id, max_stream_retries):
            check_client_disconnected(f'éæµå¼è¾…åŠ©æµ - å¾ªç¯ä¸­ ({req_id}): ')
            if isinstance(raw_data, str):
                try:
                    data = json.loads(raw_data)
                except json.JSONDecodeError:
                    logger.warning(f'[{req_id}] æ— æ³•è§£æéæµå¼æ•°æ®JSON: {raw_data}')
                    continue
            elif isinstance(raw_data, dict):
                data = raw_data
                if data.get('error') == 'rate_limit':
                    logger.warning(f"[{req_id}] ğŸš¨ éæµå¼è¯·æ±‚ä¸­æ¥æ”¶åˆ°é€Ÿç‡é™åˆ¶: {data}")
                    raise HTTPException(status_code=429, detail=f"Rate limit exceeded: {data.get('detail')}")
            else:
                logger.warning(f'[{req_id}] éæµå¼æœªçŸ¥æ•°æ®ç±»å‹: {type(raw_data)}')
                continue
            if not isinstance(data, dict):
                logger.warning(f'[{req_id}] éæµå¼æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹: {data}')
                continue
            final_data_from_aux_stream = data
            if data.get('done'):
                content = data.get('body')
                reasoning_content = data.get('reason')
                functions = data.get('function')
                break
        if final_data_from_aux_stream and final_data_from_aux_stream.get('reason') == 'internal_timeout':
            logger.error(f'[{req_id}] éæµå¼è¯·æ±‚é€šè¿‡è¾…åŠ©æµå¤±è´¥: å†…éƒ¨è¶…æ—¶')
            raise HTTPException(status_code=502, detail=f'[{req_id}] è¾…åŠ©æµå¤„ç†é”™è¯¯ (å†…éƒ¨è¶…æ—¶)')
        if final_data_from_aux_stream and final_data_from_aux_stream.get('done') is True and (content is None):
            logger.error(f'[{req_id}] éæµå¼è¯·æ±‚é€šè¿‡è¾…åŠ©æµå®Œæˆä½†æœªæä¾›å†…å®¹')
            raise HTTPException(status_code=502, detail=f'[{req_id}] è¾…åŠ©æµå®Œæˆä½†æœªæä¾›å†…å®¹')
        model_name_for_json = current_ai_studio_model_id or MODEL_NAME
        message_payload = {'role': 'assistant', 'content': content}
        finish_reason_val = 'stop'
        if functions and len(functions) > 0:
            tool_calls_list = []
            for func_idx, function_call_data in enumerate(functions):
                tool_calls_list.append({'id': f'call_{secrets.token_hex(12)}', 'index': func_idx, 'type': 'function', 'function': {'name': function_call_data['name'], 'arguments': json.dumps(function_call_data['params'])}})
            message_payload['tool_calls'] = tool_calls_list
            finish_reason_val = 'tool_calls'
            message_payload['content'] = None
        if reasoning_content:
            message_payload['reasoning_content'] = reasoning_content
        usage_stats = calculate_usage_stats([msg.model_dump() for msg in request.messages], content or '', reasoning_content)
        response_payload = {'id': f'{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}', 'object': 'chat.completion', 'created': int(time.time()), 'model': model_name_for_json, 'choices': [{'index': 0, 'message': message_payload, 'finish_reason': finish_reason_val, 'native_finish_reason': finish_reason_val}], 'usage': usage_stats}
        if not result_future.done():
            result_future.set_result(JSONResponse(content=response_payload))
        return None

async def _handle_playwright_response(req_id: str, request: ChatCompletionRequest, page: AsyncPage, context: dict, result_future: Future, submit_button_locator: Locator, check_client_disconnected: Callable) -> Optional[Tuple[Event, Locator, Callable]]:
    from server import logger
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')
    logger.info(f'[{req_id}] å®šä½å“åº”å…ƒç´ ...')
    response_container = page.locator(RESPONSE_CONTAINER_SELECTOR).last
    response_element = response_container.locator(RESPONSE_TEXT_SELECTOR)
    try:
        await expect_async(response_container).to_be_attached(timeout=20000)
        check_client_disconnected('After Response Container Attached: ')
        await expect_async(response_element).to_be_attached(timeout=90000)
        logger.info(f'[{req_id}] å“åº”å…ƒç´ å·²å®šä½ã€‚')
    except (PlaywrightAsyncError, asyncio.TimeoutError, ClientDisconnectedError) as locate_err:
        if isinstance(locate_err, ClientDisconnectedError):
            raise
        logger.error(f'[{req_id}] âŒ é”™è¯¯: å®šä½å“åº”å…ƒç´ å¤±è´¥æˆ–è¶…æ—¶: {locate_err}')
        await save_error_snapshot(f'response_locate_error_{req_id}')
        raise HTTPException(status_code=502, detail=f'[{req_id}] å®šä½AI Studioå“åº”å…ƒç´ å¤±è´¥: {locate_err}')
    except Exception as locate_exc:
        logger.exception(f'[{req_id}] âŒ é”™è¯¯: å®šä½å“åº”å…ƒç´ æ—¶æ„å¤–é”™è¯¯')
        await save_error_snapshot(f'response_locate_unexpected_{req_id}')
        raise HTTPException(status_code=500, detail=f'[{req_id}] å®šä½å“åº”å…ƒç´ æ—¶æ„å¤–é”™è¯¯: {locate_exc}')
    check_client_disconnected('After Response Element Located: ')
    if is_streaming:
        completion_event = Event()

        async def create_response_stream_generator():
            data_receiving = False
            page_controller = PageController(page, logger, req_id)
            skip_button_stop_event = asyncio.Event()
            skip_monitor_task = asyncio.create_task(page_controller.continuously_handle_skip_button(skip_button_stop_event, check_client_disconnected))
            try:
                final_content = await page_controller.get_response(check_client_disconnected)
                data_receiving = True
                lines = final_content.split('\n')
                for line_idx, line in enumerate(lines):
                    try:
                        check_client_disconnected(f'Playwrightæµå¼ç”Ÿæˆå™¨å¾ªç¯ ({req_id}): ')
                    except ClientDisconnectedError:
                        logger.info(f'[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨ä¸­æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥')
                        if data_receiving and (not completion_event.is_set()):
                            logger.info(f'[{req_id}] Playwrightæ•°æ®æ¥æ”¶ä¸­å®¢æˆ·ç«¯æ–­å¼€ï¼Œç«‹å³è®¾ç½®doneä¿¡å·')
                            completion_event.set()
                        try:
                            yield generate_sse_stop_chunk(req_id, current_ai_studio_model_id or MODEL_NAME, 'stop')
                        except Exception:
                            pass
                        break
                    if line:
                        chunk_size = STREAM_CHUNK_SIZE
                        for i in range(0, len(line), chunk_size):
                            chunk = line[i:i + chunk_size]
                            yield generate_sse_chunk(chunk, req_id, current_ai_studio_model_id or MODEL_NAME)
                            # await asyncio.sleep(0.03) # Removed artificial delay
                    if line_idx < len(lines) - 1:
                        yield generate_sse_chunk('\n', req_id, current_ai_studio_model_id or MODEL_NAME)
                        # await asyncio.sleep(0.01)
                usage_stats = calculate_usage_stats([msg.model_dump() for msg in request.messages], final_content, '')
                logger.info(f'[{req_id}] Playwrightéæµå¼è®¡ç®—çš„tokenä½¿ç”¨ç»Ÿè®¡: {usage_stats}')
                yield generate_sse_stop_chunk(req_id, current_ai_studio_model_id or MODEL_NAME, 'stop', usage_stats)
            except ClientDisconnectedError as disconnect_err:
                abort_handler = AbortSignalHandler()
                disconnect_info = abort_handler.handle_error(disconnect_err, req_id)
                logger.info(f'[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨ä¸­æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥')
                logger.info(f'[{req_id}] åœæ­¢åŸå› åˆ†æ: {disconnect_info}')
                if data_receiving and (not completion_event.is_set()):
                    logger.info(f'[{req_id}] Playwrightå®¢æˆ·ç«¯æ–­å¼€å¼‚å¸¸å¤„ç†ä¸­ç«‹å³è®¾ç½®doneä¿¡å·')
                    completion_event.set()
            except Exception as e:
                abort_handler = AbortSignalHandler()
                error_info = abort_handler.handle_error(e, req_id)
                if error_info['stop_reason'] in ['user_abort', 'client_disconnect']:
                    logger.info(f'[{req_id}] Playwrightæ£€æµ‹åˆ°åœæ­¢ä¿¡å·: {error_info}')
                    if data_receiving and (not completion_event.is_set()):
                        completion_event.set()
                else:
                    logger.error(f'[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}', exc_info=True)
                try:
                    yield generate_sse_chunk(f'\n\n[é”™è¯¯: {str(e)}]', req_id, current_ai_studio_model_id or MODEL_NAME)
                    yield generate_sse_stop_chunk(req_id, current_ai_studio_model_id or MODEL_NAME)
                except Exception:
                    pass
            finally:
                logger.info(f"[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨ç»“æŸï¼Œæ­£åœ¨åœæ­¢ 'Skip' æŒ‰é’®ç›‘æ§...")
                skip_button_stop_event.set()
                try:
                    await asyncio.wait_for(skip_monitor_task, timeout=2.0)
                    logger.info(f"[{req_id}] Playwright 'Skip' æŒ‰é’®ç›‘æ§ä»»åŠ¡å·²æˆåŠŸæ¸…ç†ã€‚")
                except asyncio.TimeoutError:
                    logger.warning(f"[{req_id}] Playwright 'Skip' æŒ‰é’®ç›‘æ§ä»»åŠ¡å…³é—­è¶…æ—¶ã€‚")
                except Exception as e_clean_skip:
                    logger.error(f"[{req_id}] æ¸…ç† Playwright 'Skip' æŒ‰é’®ç›‘æ§ä»»åŠ¡æ—¶å‡ºé”™: {e_clean_skip}")
                if not completion_event.is_set():
                    completion_event.set()
                    logger.info(f'[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨å®Œæˆäº‹ä»¶å·²è®¾ç½®')
        stream_gen_func = create_response_stream_generator()
        if not result_future.done():
            result_future.set_result(StreamingResponse(stream_gen_func, media_type='text/event-stream'))
        return (completion_event, submit_button_locator, check_client_disconnected)
    else:
        page_controller = PageController(page, logger, req_id)
        final_content = await page_controller.get_response(check_client_disconnected)
        usage_stats = calculate_usage_stats([msg.model_dump() for msg in request.messages], final_content, '')
        logger.info(f'[{req_id}] Playwrightéæµå¼è®¡ç®—çš„tokenä½¿ç”¨ç»Ÿè®¡: {usage_stats}')
        response_payload = {'id': f'{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}', 'object': 'chat.completion', 'created': int(time.time()), 'model': current_ai_studio_model_id or MODEL_NAME, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': final_content}, 'finish_reason': 'stop'}], 'usage': usage_stats}
        if not result_future.done():
            result_future.set_result(JSONResponse(content=response_payload))
        return None

async def _cleanup_request_resources(req_id: str, disconnect_check_task: Optional[asyncio.Task], completion_event: Optional[Event], result_future: Future, is_streaming: bool) -> None:
    from server import logger
    if is_streaming:
        logger.info(f'[{req_id}] æµå¼å“åº”ï¼šç›‘æ§ä»»åŠ¡å°†åœ¨ç”Ÿæˆå®Œæˆåè‡ªç„¶ç»“æŸ')
        if result_future.done() and result_future.exception() is not None:
            logger.warning(f'[{req_id}] æµå¼è¯·æ±‚å‘ç”Ÿå¼‚å¸¸ï¼Œå–æ¶ˆç›‘æ§ä»»åŠ¡')
            if disconnect_check_task and (not disconnect_check_task.done()):
                disconnect_check_task.cancel()
                try:
                    await disconnect_check_task
                except asyncio.CancelledError:
                    pass
                except Exception as task_clean_err:
                    logger.error(f'[{req_id}] æ¸…ç†å¼‚å¸¸ç›‘æ§ä»»åŠ¡æ—¶å‡ºé”™: {task_clean_err}')
        else:
            logger.info(f'[{req_id}] æ­£å¸¸æµå¼å“åº”ï¼šä¿æŒç›‘æ§ä»»åŠ¡æ´»è·ƒçŠ¶æ€')
    elif disconnect_check_task and (not disconnect_check_task.done()):
        logger.info(f'[{req_id}] éæµå¼å“åº”ï¼šå–æ¶ˆç›‘æ§ä»»åŠ¡')
        disconnect_check_task.cancel()
        try:
            await disconnect_check_task
        except asyncio.CancelledError:
            pass
        except Exception as task_clean_err:
            logger.error(f'[{req_id}] æ¸…ç†ä»»åŠ¡æ—¶å‡ºé”™: {task_clean_err}')
    logger.info(f'[{req_id}] å¤„ç†å®Œæˆã€‚')
    if is_streaming and completion_event and (not completion_event.is_set()) and (result_future.done() and result_future.exception() is not None):
        logger.warning(f'[{req_id}] æµå¼è¯·æ±‚å¼‚å¸¸ï¼Œç¡®ä¿å®Œæˆäº‹ä»¶å·²è®¾ç½®ã€‚')
        completion_event.set()

async def _process_request_refactored(req_id: str, request: ChatCompletionRequest, http_request: Request, result_future: Future) -> Optional[Tuple[Event, Locator, Callable[[str], bool]]]:
    import server
    if not hasattr(server, 'current_http_requests'):
        server.current_http_requests = {}
    server.current_http_requests[req_id] = http_request
    is_connected = await _test_client_connection(req_id, http_request)
    if not is_connected:
        from server import logger
        logger.info(f'[{req_id}]  æ ¸å¿ƒå¤„ç†å‰æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼Œæå‰é€€å‡ºèŠ‚çœèµ„æº')
        server.current_http_requests.pop(req_id, None)
        if not result_future.done():
            result_future.set_exception(HTTPException(status_code=499, detail=f'[{req_id}] å®¢æˆ·ç«¯åœ¨å¤„ç†å¼€å§‹å‰å·²æ–­å¼€è¿æ¥'))
        return None
    context = await _initialize_request_context(req_id, request)
    context = await _analyze_model_requirements(req_id, context, request)
    page = context['page']
    client_disconnected_event, disconnect_check_task, check_client_disconnected = await _setup_disconnect_monitoring(req_id, http_request, result_future, page)
    submit_button_locator = page.locator(SUBMIT_BUTTON_SELECTOR) if page else None
    completion_event = None
    try:
        await _validate_page_status(req_id, context, check_client_disconnected)
        page_controller = PageController(page, context['logger'], req_id)

        model_switch_task = asyncio.create_task(_handle_model_switching(req_id, context, check_client_disconnected))
        prep_task = asyncio.create_task(_prepare_and_validate_request(req_id, request, check_client_disconnected))
        
        context = await model_switch_task
        system_prompt, prepared_prompt, image_list = await prep_task

        await _handle_parameter_cache(req_id, context)
        await page_controller.adjust_parameters(request.model_dump(exclude_none=True), context['page_params_cache'], context['params_cache_lock'], context['model_id_to_use'], context['parsed_model_list'], check_client_disconnected)
        await page_controller.set_system_instructions(system_prompt, check_client_disconnected)
        check_client_disconnected('æäº¤æç¤ºå‰æœ€ç»ˆæ£€æŸ¥')
        await page_controller.submit_prompt(prepared_prompt, image_list, check_client_disconnected)
        response_result = await _handle_response_processing(req_id, request, page, context, result_future, submit_button_locator, check_client_disconnected, disconnect_check_task)
        if response_result:
            completion_event, _, _ = response_result
        return (completion_event, submit_button_locator, check_client_disconnected)
    except ClientDisconnectedError as disco_err:
        context['logger'].info(f'[{req_id}] æ•è·åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ä¿¡å·: {disco_err}')
        if not result_future.done():
            result_future.set_exception(HTTPException(status_code=499, detail=f'[{req_id}] Client disconnected during processing.'))
    except HTTPException as http_err:
        context['logger'].warning(f'[{req_id}] æ•è·åˆ° HTTP å¼‚å¸¸: {http_err.status_code} - {http_err.detail}')
        if not result_future.done():
            result_future.set_exception(http_err)
    except PlaywrightAsyncError as pw_err:
        context['logger'].error(f'[{req_id}] æ•è·åˆ° Playwright é”™è¯¯: {pw_err}')
        await save_error_snapshot(f'process_playwright_error_{req_id}')
        if not result_future.done():
            result_future.set_exception(HTTPException(status_code=502, detail=f'[{req_id}] Playwright interaction failed: {pw_err}'))
    except Exception as e:
        context['logger'].exception(f'[{req_id}] æ•è·åˆ°æ„å¤–é”™è¯¯')
        await save_error_snapshot(f'process_unexpected_error_{req_id}')
        if not result_future.done():
            result_future.set_exception(HTTPException(status_code=500, detail=f'[{req_id}] Unexpected server error: {e}'))
    finally:
        request_manager.unregister_request(req_id)
        await _cleanup_request_resources(req_id, disconnect_check_task, completion_event, result_future, request.stream)
